{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from datetime import date, timedelta, datetime\n",
    "from itertools import chain, permutations\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from numpy import matrix\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from time import time,strftime, gmtime\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.datasets import Reddit\n",
    "import bisect\n",
    "import functools\n",
    "import glob\n",
    "import math\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mit\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import statsmodels as sm\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.loc[df['l_veh'].notna()]\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir','movements']).count().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','pair']]\n",
    "dg = dg.rename(columns={\"pair\":\"approach_obs\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left')\n",
    "df = df[df[\"movements\"]==df[\"approach_obs\"]]\n",
    "df['h_veh'] = df['h_veh'].fillna(0)\n",
    "df['count'] = df['h_veh'] + df['l_veh']\n",
    "df = df.drop(['l_veh','h_veh'],axis=1)\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir']).sum().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','count']]\n",
    "dg = dg.rename(columns={\"count\":\"approach_count\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').drop(['approach_obs'],axis=1)\n",
    "# df = df[df['speed_up'].notna()]\n",
    "# df = df[df['speed_dn'].notna()]\n",
    "df['date'] = pd.to_datetime(df['measurement_tstamp']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['pair','measurement_tstamp','count','approach_count','date']].merge(pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None), how='right', on=['pair','measurement_tstamp'])\n",
    "df = df.drop(['Unnamed: 0','l_veh','h_veh','movements'],axis=1)\n",
    "df[['dir','move']] = df.Direction.str.split(\"_\",expand=True)\n",
    "df = df.merge(pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Allowable_Movements.csv\", delimiter=',', dtype=None).rename(columns={'movements':'allow_movements'}), on=['Detector_ID','dir'], how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inter_list = list([6353,6354,6351,6814])\n",
    "# df = df[df['Detector_ID'].isin(inter_list)]\n",
    "df['sum_index'] = 0\n",
    "df.loc[(df['Detector_ID']==6351)&((df['Direction']=='WB_RT')|(df['Direction']=='NB_TH')),'sum_index']=1\n",
    "df.loc[(df['Detector_ID']==6814)&((df['Direction']=='WB_LT')|(df['Direction']=='EB_RT')),'sum_index']=2\n",
    "df.loc[(df['Detector_ID']==6354)&((df['Direction']=='WB_RT')|(df['Direction']=='NB_TH')|(df['Direction']=='EB_LT')),'sum_index']=3\n",
    "df.loc[(df['Detector_ID']==6351)&((df['Direction']=='WB_LT')|(df['Direction']=='EB_RT')|(df['Direction']=='SB_TH')),'sum_index']=4\n",
    "df.loc[(df['Detector_ID']==6353)&((df['Direction']=='WB_RT')|(df['Direction']=='NB_TH')|(df['Direction']=='EB_LT')),'sum_index']=5\n",
    "df.loc[(df['Detector_ID']==6354)&((df['Direction']=='WB_LT')|(df['Direction']=='EB_RT')|(df['Direction']=='SB_TH')),'sum_index']=6\n",
    "df['nan_exist'] = 0\n",
    "df.loc[df['count'].isna(),'nan_exist']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df[['measurement_tstamp','count','sum_index','nan_exist']].groupby(['measurement_tstamp','sum_index']).sum().reset_index()\n",
    "dg = dg[dg['sum_index']>0]\n",
    "dg = dg[dg['nan_exist']==0]\n",
    "dg = dg.rename(columns={'count':'approach_count_dg','sum_index':'det_index'})\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sum_index']=0\n",
    "df.loc[(df['Detector_ID']==6814)&(df['dir']=='NB'),'det_index']=1\n",
    "df.loc[(df['Detector_ID']==6351)&(df['dir']=='SB'),'det_index']=2\n",
    "df.loc[(df['Detector_ID']==6351)&(df['dir']=='NB'),'det_index']=3\n",
    "df.loc[(df['Detector_ID']==6354)&(df['dir']=='SB'),'det_index']=4\n",
    "df.loc[(df['Detector_ID']==6354)&(df['dir']=='NB'),'det_index']=5\n",
    "df.loc[(df['Detector_ID']==6353)&(df['dir']=='SB'),'det_index']=6\n",
    "print(df['approach_count'].isna().sum(axis=0))\n",
    "df = df.merge(dg[['measurement_tstamp','det_index','approach_count_dg']], on=['measurement_tstamp','det_index'], how='left')\n",
    "df.approach_count.fillna(df.approach_count_dg, inplace=True)\n",
    "df.drop(['approach_count_dg'], inplace=True, axis=1)\n",
    "print(df['approach_count'].isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['avg_speed_up'].isna().sum(axis=0))\n",
    "print(df['avg_speed_dn'].isna().sum(axis=0))\n",
    "print(df['ref_speed_up'].isna().sum(axis=0))\n",
    "print(df['ref_speed_dn'].isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = df[['Up','year','month','weekday','hour','avg_speed_up','ref_speed_up','avg_speed_dn',\n",
    "         'ref_speed_dn']].groupby(['Up','year','month','weekday','hour']).mean()\n",
    "dq = dq.rename(columns={'avg_speed_up':\"avg_speed_up_dq\",'ref_speed_up':'ref_speed_up_dq','avg_speed_dn':'avg_speed_dn_dq',\n",
    "                        'ref_speed_dn':'ref_speed_dn_dq'})\n",
    "df = df.merge(dq,on=['Up','year','month','weekday','hour'])\n",
    "df.avg_speed_up.fillna(df.avg_speed_up_dq, inplace=True)\n",
    "df.avg_speed_dn.fillna(df.avg_speed_dn_dq, inplace=True)\n",
    "df.ref_speed_up.fillna(df.ref_speed_up_dq, inplace=True)\n",
    "df.ref_speed_dn.fillna(df.ref_speed_dn_dq, inplace=True)\n",
    "\n",
    "\n",
    "print(df['avg_speed_up'].isna().sum(axis=0))\n",
    "print(df['avg_speed_dn'].isna().sum(axis=0))\n",
    "print(df['ref_speed_up'].isna().sum(axis=0))\n",
    "print(df['ref_speed_dn'].isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_list = list([6353,6354,6351,6814])\n",
    "dg = df[df['Detector_ID'].isin(inter_list)]\n",
    "print(dg['speed_up'].isna().sum(axis=0))\n",
    "print(dg['speed_dn'].isna().sum(axis=0))\n",
    "print(dg['avg_speed_up'].isna().sum(axis=0))\n",
    "print(dg['avg_speed_dn'].isna().sum(axis=0))\n",
    "print(dg['ref_speed_up'].isna().sum(axis=0))\n",
    "print(dg['ref_speed_dn'].isna().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Data_all.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from datetime import date, timedelta, datetime\n",
    "from itertools import chain, permutations\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from numpy import matrix\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from time import time,strftime, gmtime\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.datasets import Reddit\n",
    "import bisect\n",
    "import functools\n",
    "import glob\n",
    "import math\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mit\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import statsmodels as sm\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szahedi1\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (5,19,23,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Data_all.csv\", delimiter=',' ,dtype=None)\n",
    "dq = df[df.avg_speed_up.notna()].drop(['pair','count','date', 'Up','Dn','Direction','Exclusive_Lane',\n",
    "                                       'roadname_up','xd_up','tmc_up', 'roadname_dn','xd_dn','tmc_dn',\n",
    "                                       'lanes_dn','speed_dn','avg_speed_dn','ref_speed_dn','Intersection Name',\n",
    "                                       'sum_index','nan_exist','det_index','move','avg_speed_up_dq',\n",
    "                                       'ref_speed_up_dq','avg_speed_dn_dq','ref_speed_dn_dq','measurement_tstamp',\n",
    "                                      'approach_count','speed_up','ref_speed_up','Unnamed: 0'],axis=1)\n",
    "dum_dq = pd.get_dummies(dq, columns=['year','month','weekday','day',\"hour\",'quarter','Detector_ID','dir','lanes_up' ,\n",
    "                                     'allow_movements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dq = dum_dq+0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import keras\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "# from tensorflow.keras.layers import Activation\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                6848      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4096      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4096      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4096      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 20,225\n",
      "Trainable params: 19,713\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(2048, 107), b.shape=(107, 64), m=2048, n=64, k=107\n\t [[node sequential/dense/MatMul (defined at <ipython-input-5-5d96baf5106f>:43) ]] [Op:__inference_train_function_2057]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5d96baf5106f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(2048, 107), b.shape=(107, 64), m=2048, n=64, k=107\n\t [[node sequential/dense/MatMul (defined at <ipython-input-5-5d96baf5106f>:43) ]] [Op:__inference_train_function_2057]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "result = pd.DataFrame(columns = [\"Train_MSE\",\"Validation_MSE\",\"Train_MAE\",\"Validation_MAE\",\n",
    "                                 \"Train_MAPE\",\"Validation_MAPE\",\"Train_R_Squared\",\"Validation_R_Squared\"])\n",
    "Epochs=2\n",
    "\n",
    "train, test = train_test_split(dum_dq, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "X_train=train.drop(['avg_speed_up'],axis=1)\n",
    "X_test=test.drop(['avg_speed_up'],axis=1)\n",
    "\n",
    "y_train=train['avg_speed_up']\n",
    "y_test=test['avg_speed_up']\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(64, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(64, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(64, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mse','mae','mape'])\n",
    "history = model.fit(X_train, y_train, epochs=Epochs, batch_size=2048,  verbose=1, validation_data = (X_test, y_test))\n",
    "\n",
    "\n",
    "result = result.append({\"Train_MSE\": history.history['mse'][Epochs-1],\n",
    "                        \"Validation_MSE\": history.history['val_mse'][Epochs-1],\n",
    "                        \"Train_MAE\": history.history['mae'][Epochs-1],\n",
    "                        \"Validation_MAE\": history.history['val_mae'][Epochs-1],\n",
    "                        \"Train_MAPE\": history.history['mape'][Epochs-1],\n",
    "                        \"Validation_MAPE\": history.history['val_mape'][Epochs-1],\n",
    "                        \"Training_Set_Size\": X_train.shape[0]},\n",
    "                       ignore_index=True)\n",
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dq.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Up']==1595153899)&(df['year']==2020)&(df['weekday']==0)&(df['hour']==5)].avg_speed_up.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Up']==1595153899)&(df['year']==2020)&(df['weekday']==0)&(df['hour']==5)&(df['avg_speed_up']==11)].month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df[~df['approach_count'].isna()].drop(['pair','count','date', 'Up','Dn','Direction','Exclusive_Lane',\n",
    "                                            'roadname_up','xd_up','tmc_up', 'roadname_dn','xd_dn','tmc_dn',\n",
    "                                            'lanes_dn','speed_dn','avg_speed_dn','ref_speed_dn','Intersection Name',\n",
    "                                            'sum_index','nan_exist','det_index','move'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.drop_duplicates()\n",
    "dg['measurement_tstamp'] = pd.to_datetime(dg.measurement_tstamp)\n",
    "dg['week'] = dg.measurement_tstamp.dt.week\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_dg = pd.get_dummies(dg, columns=[\"hour\",'quarter','year','month','weekday','day','Detector_ID','dir','lanes_up' ,'allow_movements'])\n",
    "dq = dg[['Detector_ID','year']]\n",
    "dum_dg = dum_dg.join(dq).drop(['measurement_tstamp','week','year'],axis=1)\n",
    "dum_dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    train=dum_dg.loc[dum_dg['Detector_ID']!=loc]\n",
    "    testt=dum_dg.loc[dum_dg['Detector_ID']==loc]\n",
    "    trainn, test = train_test_split(testt, test_size=0.1, random_state=42, shuffle=True)\n",
    "    train=train.append(trainn)\n",
    "    \n",
    "    \n",
    "    X_train=train.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    X_test=test.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    \n",
    "    y_train=train['approach_count']\n",
    "    y_test=test['approach_count']\n",
    "    \n",
    "    regressor = xgb.XGBRegressor(n_estimators=100,reg_lambda=1, gamma=0,max_depth=3)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n",
    "    if (i==1):\n",
    "        results=res\n",
    "    else:\n",
    "        results=pd.concat([results,res])\n",
    "if (len(results.loc[results['actual']-results['approach_count']!=0])==0):\n",
    "    print('Done')\n",
    "else:\n",
    "    print('there is a problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAPE\"])\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    result=result.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'approach_count'], results.loc[results['Detector_ID']==loc,'pred'] ),\n",
    "                        \"XGBoost_MAPE\": (abs(results.loc[results['Detector_ID']==loc,'approach_count']-results.loc[results['Detector_ID']==loc,'pred'])/results.loc[results['Detector_ID']==loc,'approach_count']).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)\n",
    "    \n",
    "results['pro_act'] = results['approach_count']/results['approach_count']\n",
    "results['pro_est'] = results['pred']/results['approach_count']\n",
    "\n",
    "# proportion\n",
    "result_p = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAE\"])\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    result_p=result_p.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'pro_act'], results.loc[results['Detector_ID']==loc,'pro_est'] ),\n",
    "                        \"XGBoost_MAE\": (abs(results.loc[results['Detector_ID']==loc,'pro_act']-results.loc[results['Detector_ID']==loc,'pro_est'])).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach Volume Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None)\n",
    "\n",
    "df = df.loc[df['l_veh'].notna()]\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir','movements']).count().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','pair']]\n",
    "dg = dg.rename(columns={\"pair\":\"approach_obs\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left')\n",
    "df = df[df[\"movements\"]==df[\"approach_obs\"]]\n",
    "df['h_veh'] = df['h_veh'].fillna(0)\n",
    "df['count'] = df['h_veh'] + df['l_veh']\n",
    "df = df.drop(['l_veh','h_veh'],axis=1)\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir']).sum().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','count']]\n",
    "dg = dg.rename(columns={\"count\":\"approach_count\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').drop(['approach_obs'],axis=1)\n",
    "# df = df[df['speed_up'].notna()]\n",
    "# df = df[df['speed_dn'].notna()]\n",
    "df['date'] = pd.to_datetime(df['measurement_tstamp']).dt.date\n",
    "\n",
    "df = df[['pair','measurement_tstamp','count','approach_count','date']].merge(pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None), how='right', on=['pair','measurement_tstamp'])\n",
    "df = df.drop(['Unnamed: 0','l_veh','h_veh','movements'],axis=1)\n",
    "df[['dir','move']] = df.Direction.str.split(\"_\",expand=True)\n",
    "df = df.merge(pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Allowable_Movements.csv\", delimiter=',', dtype=None).rename(columns={'movements':'allow_movements'}), on=['Detector_ID','dir'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df[~df['approach_count'].isna()].drop(['pair','count','date', 'Up','Dn','Direction','Exclusive_Lane',\n",
    "                                            'roadname_up','xd_up','tmc_up', 'roadname_dn','xd_dn','tmc_dn',\n",
    "                                            'lanes_dn','speed_dn','avg_speed_dn','ref_speed_dn','Intersection Name',\n",
    "                                            'move'],axis=1)\n",
    "dg = dg[~dg['speed_up'].isna()]\n",
    "dg = dg.drop_duplicates()\n",
    "dg['measurement_tstamp'] = pd.to_datetime(dg.measurement_tstamp)\n",
    "dg['week'] = dg.measurement_tstamp.dt.week\n",
    "\n",
    "dum_dg = pd.get_dummies(dg, columns=[\"hour\",'quarter','year','month','weekday','day','Detector_ID','dir','lanes_up' ,\n",
    "                                     'allow_movements'])\n",
    "dq = dg[['Detector_ID','year']]\n",
    "dum_dg = dum_dg.join(dq).drop(['measurement_tstamp','week','year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    train=dum_dg.loc[dum_dg['Detector_ID']!=loc]\n",
    "    testt=dum_dg.loc[dum_dg['Detector_ID']==loc]\n",
    "    trainn, test = train_test_split(testt, test_size=0.1, random_state=42, shuffle=True)\n",
    "    train=train.append(trainn)\n",
    "    \n",
    "    \n",
    "    X_train=train.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    X_test=test.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    \n",
    "    y_train=train['approach_count']\n",
    "    y_test=test['approach_count']\n",
    "    \n",
    "    regressor = xgb.XGBRegressor(n_estimators=100,reg_lambda=1, gamma=0,max_depth=3)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n",
    "    if (i==1):\n",
    "        results=res\n",
    "    else:\n",
    "        results=pd.concat([results,res])\n",
    "if (len(results.loc[results['actual']-results['approach_count']!=0])==0):\n",
    "    print('Done')\n",
    "else:\n",
    "    print('there is a problem')\n",
    "    \n",
    "# count\n",
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAPE\"])\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    result=result.append({\"Detector_ID\":str(loc),\n",
    "                          \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'approach_count'],\n",
    "                                                       \"XGBoost_MAPE\": (abs(results.loc[results['Detector_ID']==loc,\n",
    "                                                                                        'approach_count']-results.loc[results['Detector_ID']==loc,\n",
    "                                                                                                                      'pred'])/results.loc[results['Detector_ID']==loc,\n",
    "                                                                                                                                           'approach_count']).mean()*100},\n",
    "                                                       ignore_index=True)\n",
    "    \n",
    "results['pro_act'] = results['approach_count']/results['approach_count']\n",
    "results['pro_est'] = results['pred']/results['approach_count']\n",
    "\n",
    "# proportion\n",
    "result_p = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAE\"])\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    result_p=result_p.append({\"Detector_ID\":str(loc),\n",
    "                              \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'pro_act'],\n",
    "                                                           \"XGBoost_MAE\": (abs(results.loc[results['Detector_ID']==loc,\n",
    "                                                                                           'pro_act']-results.loc[results['Detector_ID']==loc,'pro_est'])).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None)\n",
    "\n",
    "df = df.loc[df['l_veh'].notna()]\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir','movements']).count().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','pair']]\n",
    "dg = dg.rename(columns={\"pair\":\"approach_obs\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left')\n",
    "df = df[df[\"movements\"]==df[\"approach_obs\"]]\n",
    "df['h_veh'] = df['h_veh'].fillna(0)\n",
    "df['count'] = df['h_veh'] + df['l_veh']\n",
    "df = df.drop(['l_veh','h_veh'],axis=1)\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir']).sum().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','count']]\n",
    "dg = dg.rename(columns={\"count\":\"approach_count\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').drop(['approach_obs'],axis=1)\n",
    "# df = df[df['speed_up'].notna()]\n",
    "# df = df[df['speed_dn'].notna()]\n",
    "df['date'] = pd.to_datetime(df['measurement_tstamp']).dt.date\n",
    "\n",
    "df = df[['pair','measurement_tstamp','count','approach_count','date']].merge(pd.read_csv(\"D:/Amir_Nohekhan/data.csv\", delimiter=',' ,dtype=None), how='right', on=['pair','measurement_tstamp'])\n",
    "df = df.drop(['Unnamed: 0','l_veh','h_veh','movements'],axis=1)\n",
    "df[['dir','move']] = df.Direction.str.split(\"_\",expand=True)\n",
    "df = df.merge(pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Allowable_Movements.csv\", delimiter=',', dtype=None).rename(columns={'movements':'allow_movements'}), on=['Detector_ID','dir'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df[~df['approach_count'].isna()].drop(['pair','count','date', 'Up','Dn','Direction','Exclusive_Lane',\n",
    "                                            'roadname_up','xd_up','tmc_up', 'roadname_dn','xd_dn','tmc_dn',\n",
    "                                            'lanes_dn','speed_dn','avg_speed_dn','ref_speed_dn','Intersection Name',\n",
    "                                            'move'],axis=1)\n",
    "dg = dg[~dg['speed_up'].isna()]\n",
    "dg = dg[~dg['avg_speed_up'].isna()]\n",
    "\n",
    "dg = dg.drop_duplicates()\n",
    "dg['measurement_tstamp'] = pd.to_datetime(dg.measurement_tstamp)\n",
    "dg['week'] = dg.measurement_tstamp.dt.week\n",
    "\n",
    "dum_dg = pd.get_dummies(dg, columns=[\"hour\",'quarter','year','month','weekday','day','Detector_ID','dir','lanes_up' ,\n",
    "                                     'allow_movements'])\n",
    "dq = dg[['Detector_ID','year']]\n",
    "dum_dg = dum_dg.join(dq).drop(['measurement_tstamp','week','year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg[(dg['Detector_ID']==7047)&(dg['dir']==\"NB\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import keras\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "# from tensorflow.keras.layers import Activation\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, BatchNormalization, LeakyReLU\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#from keras.layers import LeakyReLU\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "from random import randint\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns = [\"Intersection_ID\",\"Train_MSE\",\"Validation_MSE\",\"Train_MAE\",\"Validation_MAE\",\n",
    "                                 \"Train_MAPE\",\"Validation_MAPE\",\"Train_R_Squared\",\"Validation_R_Squared\"])\n",
    "Epochs=20\n",
    "i=0\n",
    "for loc in dum_dg.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    train=dum_dg.loc[dum_dg['Detector_ID']!=loc]\n",
    "    testt=dum_dg.loc[dum_dg['Detector_ID']==loc]\n",
    "    trainn, test = train_test_split(testt, test_size=0.1, random_state=42, shuffle=True)\n",
    "    train=train.append(trainn)\n",
    "    \n",
    "    \n",
    "    X_train=train.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    X_test=test.drop(['approach_count','Detector_ID'],axis=1)\n",
    "    \n",
    "    y_train=train['approach_count']\n",
    "    y_test=test['approach_count']\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train= scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=110, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(128, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(128, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(128, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mse','mae','mape'])\n",
    "    history = model.fit(X_train, y_train, epochs=Epochs, batch_size=128,  verbose=1, validation_data = (X_test, y_test))\n",
    "    \n",
    "    \n",
    "    result = result.append({\"Intersection_ID\": loc,\n",
    "                            \"Train_MSE\": history.history['mse'][Epochs-1],\n",
    "                            \"Validation_MSE\": history.history['val_mse'][Epochs-1],\n",
    "                            \"Train_MAE\": history.history['mae'][Epochs-1],\n",
    "                            \"Validation_MAE\": history.history['val_mae'][Epochs-1],\n",
    "                            \"Train_MAPE\": history.history['mape'][Epochs-1],\n",
    "                            \"Validation_MAPE\": history.history['val_mape'][Epochs-1],\n",
    "                            \"Training_Set_Size\": X_train.shape[0]},\n",
    "                           ignore_index=True)\n",
    "    print(history.history.keys())\n",
    "    # \"Loss\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Detector_ID']==6814].groupby(['Detector_ID','dir']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['Detector_ID']==6351].groupby(['Detector_ID','dir']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dn_dir'] = ['NB' if x ==\"NB_TH\" else 'NB' if x ==\"WB_RT\" else 'NB' if x ==\"EB_LT\" else\n",
    "                'SB' if x ==\"SB_TH\" else 'SB' if x ==\"EB_RT\" else 'SB' if x ==\"WB_LT\" else\n",
    "                'EB' if x ==\"EB_TH\" else 'EB' if x ==\"NB_RT\" else 'EB' if x ==\"SB_LT\" else 'WB'for x in df[\"Direction\"]]\n",
    "dg = df.groupby(['Detector_ID','measurement_tstamp','dn_dir']).count().reset_index()\n",
    "dg = dg[dg['pair']==3][['Detector_ID','measurement_tstamp','dn_dir']]\n",
    "df = df.merge(dg, how='inner', on=['Detector_ID','measurement_tstamp','dn_dir'])\n",
    "dg = df.groupby(['Detector_ID','measurement_tstamp','dn_dir']).sum().reset_index()\n",
    "dg = dg[['Detector_ID','measurement_tstamp','dn_dir','count']].rename(columns={'count':'dn_count'})\n",
    "df = df.merge(dg, how='inner', on=['Detector_ID','measurement_tstamp','dn_dir'])\n",
    "df = df.dropna()\n",
    "df['measurement_tstamp'] = pd.to_datetime(df.measurement_tstamp)\n",
    "df['week'] = df.measurement_tstamp.dt.week\n",
    "\n",
    "dg = df.groupby(['Detector_ID','year','week']).count().reset_index()\n",
    "dh = dg.groupby(['Detector_ID']).idxmax().reset_index()\n",
    "for i in range(dh.shape[0]):\n",
    "    dh.iloc[i,1] = dg.iloc[dh.iloc[i,3],1]\n",
    "    dh.iloc[i,2] = dg.iloc[dh.iloc[i,3],2]\n",
    "    \n",
    "dh = dh[['Detector_ID','year','week']]\n",
    "dh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df = pd.get_dummies(df, columns=[\"hour\",'quarter','year','month','weekday','day','week',\n",
    "                                     'Detector_ID','dir','move','lanes_up','lanes_dn'] )\n",
    "dq = df[['Detector_ID','year','week']]\n",
    "dum_df = dum_df.join(dq).drop(['roadname_up','roadname_dn','pair','measurement_tstamp','Up','Dn','Direction','xd_up','tmc_up','xd_dn','tmc_dn',\n",
    "                               'Intersection Name','date','dn_dir','movements'],axis=1)\n",
    "dum_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    train=dum_df.loc[dum_df['Detector_ID']!=loc]\n",
    "    train=train.append(dum_df.loc[(dum_df['Detector_ID']==loc)&\n",
    "                              (dum_df['year']==dh.loc[dh['Detector_ID']==loc,'year'].item())&\n",
    "                              (dum_df['week']==dh.loc[dh['Detector_ID']==loc,'week'].item())])\n",
    "    test=dum_df.loc[dum_df['Detector_ID']==loc]\n",
    "    \n",
    "    X_train=train.drop(['count','Detector_ID','year','week'],axis=1)\n",
    "    X_test=test.drop(['count','Detector_ID','year','week'],axis=1)\n",
    "    \n",
    "    y_train=train['count']\n",
    "    y_test=test['count']\n",
    "    \n",
    "    regressor = xgb.XGBRegressor(n_estimators=100,reg_lambda=1, gamma=0,max_depth=3)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n",
    "    if (i==1):\n",
    "        results=res\n",
    "    else:\n",
    "        results=pd.concat([results,res])\n",
    "if (len(results.loc[results['actual']-results['count']!=0])==0):\n",
    "    print('Done')\n",
    "else:\n",
    "    print('there is a problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAPE\"])\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    result=result.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'count'], results.loc[results['Detector_ID']==loc,'pred'] ),\n",
    "                        \"XGBoost_MAPE\": (abs(results.loc[results['Detector_ID']==loc,'count']-results.loc[results['Detector_ID']==loc,'pred'])/results.loc[results['Detector_ID']==loc,'count']).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)\n",
    "    \n",
    "results['pro_act'] = results['count']/results['approach_count']\n",
    "results['pro_est'] = results['pred']/results['approach_count']\n",
    "\n",
    "# proportion\n",
    "result_p = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAE\"])\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    result_p=result_p.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'pro_act'], results.loc[results['Detector_ID']==loc,'pro_est'] ),\n",
    "                        \"XGBoost_MAE\": (abs(results.loc[results['Detector_ID']==loc,'pro_act']-results.loc[results['Detector_ID']==loc,'pro_est'])).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = pd.DataFrame()\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    dn=dn.append(dum_df.loc[(dum_df['Detector_ID']==loc)&\n",
    "                            (dum_df['year']==dh.loc[dh['Detector_ID']==loc,'year'].item())&\n",
    "                            (dum_df['week']==dh.loc[dh['Detector_ID']==loc,'week'].item())])\n",
    "dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['count_diff']    = results['pred']-results['count']\n",
    "results['count_diff_h0'] = (results['approach_count']*results['pro_m'])-results['count']\n",
    "results['abs_count_diff']    = abs(results['count_diff'])\n",
    "results['abs_count_diff_h0'] = abs(results['count_diff_h0'])\n",
    "results.abs_count_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.abs_count_diff_h0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['Detector_ID','count_diff','abs_count_diff','abs_count_diff_h0','count']].groupby(['Detector_ID']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dh = dh[['Detector_ID','measurement_tstamp']]\n",
    "dh = dh.rename(columns={\"measurement_tstamp\":\"date\"})\n",
    "dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dh.merge(df, how='inner', on=['Detector_ID','date'])\n",
    "dh['pro'] = dh['count']/dh['approach_count']\n",
    "dh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh[(dh['Detector_ID']==7047) & (dh['Direction']=='WB_RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dh.rename(columns={'count':'count_m', 'approach_count':'approach_count_m', 'dn_count':'dn_count_m', 'pro':'pro_m'})\n",
    "df = df.merge(dh[['Detector_ID','pair','hour','quarter','count_m', 'approach_count_m', 'dn_count_m', 'pro_m']], how='left'\n",
    "              , on=['Detector_ID','pair','hour','quarter'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['count_est'] = df['approach_count']*df['pro']\n",
    "df['mape'] = abs(df['count_est']-df['count'])/df['count']*100\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['mape','move']].groupby(['move']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['pair']=='(1595162956,1595392997)') & (df['hour']==8) & (df['quarter']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_list = [6353,6354,6351,6814]\n",
    "dg = df[df.Detector_ID.isin(det_list)]\n",
    "dg = dg.groupby(['Detector_ID','measurement_tstamp']).count().reset_index()\n",
    "dg['amax'] = [6 if x == 6814 else 6 if x == 6351 else 12 for x in dg[\"Detector_ID\"]]\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_list = [6353,6354,6351,6814]\n",
    "dg = df[df.Detector_ID.isin(det_list)]\n",
    "dg = dg.groupby(['measurement_tstamp']).count().reset_index()\n",
    "dg['pair'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_list = [6353,6354,6351]\n",
    "dg = df[df.Detector_ID.isin(det_list)]\n",
    "dg = dg.groupby(['measurement_tstamp']).count().reset_index()\n",
    "dg['pair'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_list = [6353,6354]\n",
    "dg = df[df.Detector_ID.isin(det_list)]\n",
    "dg = dg.groupby(['measurement_tstamp']).count().reset_index()\n",
    "dg['pair'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Detector_ID']==6354) & (df['Direction']=='NB_TH') & (df['year']==2018) & (df['month']==5) & (df['day']==4)].to_csv(\"C:\\\\Users\\\\szahedi1\\\\Desktop\\\\6354.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Detector_ID']==6354) & (df['Direction']=='NB_TH') & (df['year']==2019) & (df['month']==8)].day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date='2018-05-04'\n",
    "df.loc[(df['Detector_ID']==6354)&(df['date'].astype(str)==date) & (df['Direction']=='NB_RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector_ID = 7047\n",
    "Direction = 'NB_LT'\n",
    "list_date = df.loc[(df['Detector_ID']==Detector_ID) & (df['Direction']==Direction)].date.unique()\n",
    "hue_order=list_date[1:50]\n",
    "#palette = sns.color_palette(\"Spectral\", len(hue_order))\n",
    "fig = plt.figure(figsize=(40,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.lineplot(x='time', y='count', hue='date', hue_order=hue_order, data= df.loc[(df['Detector_ID']==Detector_ID) & (df['date'].isin(hue_order)) & (df['Direction']==Direction)]\n",
    "             , legend='full' , ci=None, ax=ax1) #,palette=palette\n",
    "ax1.set_xlim(0,24)\n",
    "ax1.set_title('Detector '+str(Detector_ID), fontweight='bold')\n",
    "ax1.set_ylabel('Flow (veh/15-min)')\n",
    "ax1.set_xlabel('Time of Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector_ID = 7047\n",
    "Direction = 'NB_TH'\n",
    "list_date = df.loc[(df['Detector_ID']==Detector_ID) & (df['Direction']==Direction)].date.unique()\n",
    "hue_order=list_date[20:40]\n",
    "#palette = sns.color_palette(\"Spectral\", len(hue_order))\n",
    "fig = plt.figure(figsize=(40,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.lineplot(x='time', y='count', hue='date', hue_order=hue_order, data= df.loc[(df['Detector_ID']==Detector_ID) & (df['date'].isin(hue_order)) & (df['Direction']==Direction)]\n",
    "             , estimator='mean',legend='full' , ci=None, ax=ax1) #,palette=palette\n",
    "ax1.set_xlim(0,24)\n",
    "ax1.set_title('Detector '+str(Detector_ID), fontweight='bold')\n",
    "ax1.set_ylabel('Flow (veh/15-min)')\n",
    "ax1.set_xlabel('Time of Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detector_ID = 7047\n",
    "Direction = 'NB_RT'\n",
    "list_date = df.loc[(df['Detector_ID']==Detector_ID) & (df['Direction']==Direction)].date.unique()\n",
    "hue_order=list_date[20:23]\n",
    "#palette = sns.color_palette(\"Spectral\", len(hue_order))\n",
    "fig = plt.figure(figsize=(40,10))\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.lineplot(x='time', y='count', hue='date', hue_order=hue_order, data= df.loc[(df['Detector_ID']==Detector_ID) & (df['date'].isin(hue_order)) & (df['Direction']==Direction)]\n",
    "             , estimator='mean',legend='full' , ci=None, ax=ax1) #,palette=palette\n",
    "ax1.set_xlim(0,24)\n",
    "ax1.set_title('Detector '+str(Detector_ID), fontweight='bold')\n",
    "ax1.set_ylabel('Flow (veh/15-min)')\n",
    "ax1.set_xlabel('Time of Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date='2019-11-21'\n",
    "Detector_ID = 7047\n",
    "Direction = 'NB_LT'\n",
    "\n",
    "list_date = df.loc[(df['Detector_ID']==Detector_ID) & (df['Direction']==Direction)].date.unique()\n",
    "df['time'] = df['hour']+df['quarter']/4\n",
    "fig, ax1 = plt.subplots(figsize=(15,10))\n",
    "i=0\n",
    "for date in list_date:\n",
    "    if i>3:\n",
    "        break\n",
    "    else:\n",
    "        df.loc[(df['Detector_ID']==Detector_ID)&(df['date'].astype(str)==date) & (df['Direction']==Direction)].plot(ax=ax1,x='time', y='count',color='mediumseagreen',legend=None)\n",
    "        i+=1\n",
    "#dg.loc[dg['Station']==i].plot(ax=ax,x='hour', y='q75')\n",
    "ax1.set_xlabel(\"Time of Day\",size=15)\n",
    "ax1.set_ylabel('Flow (veh/15-min)', color='black',size=15)\n",
    "#     ax1.set_xticks(np.arange(0, 97, 4))\n",
    "#     ax1.set_xticklabels(tt, rotation=90, fontsize=25)\n",
    "ax1.set_xlim(0,24)\n",
    "#plt.gcf().autofmt_xdate()\n",
    "#ax1.xaxis.set_major_formatter(myFmt)\n",
    "#plt.legend([bpr[\"boxes\"][0],bpl[\"boxes\"][0]], ['ANN', 'FSTGCN'], edgecolor='black', loc=9, prop={'size': 15})\n",
    "#plt.legend(prop={'size': 15})\n",
    "#ax1.legend(['Through','Left-Turn','Right-Turn'], prop={'size': 15})\n",
    "plt.title('Detector '+str(Detector_ID),size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFmt = mdates.DateFormatter('%H:%M:%S')\n",
    "\n",
    "for i in tmc.tmc.unique().tolist():\n",
    "    date='2019-02-20'\n",
    "    fig, ax1 = plt.subplots(figsize=(15,10))\n",
    "    #dg.loc[dg['Station']==i].plot(ax=ax,x='hour', y='q25')\n",
    "    tmc.loc[(tmc['tmc']==i)&(tmc['date']==date)].plot(ax=ax1,x='time', y='FSTGCN',color='mediumseagreen',legend=None)\n",
    "    tmc.loc[(tmc['tmc']==i)&(tmc['date']==date)].plot(ax=ax1,x='time', y='ANN', color='salmon',legend=None)\n",
    "    tmc.loc[(tmc['tmc']==i)&(tmc['date']==date)].plot(ax=ax1,x='time', y='XGBoost', color='cornflowerblue',legend=None)\n",
    "    tmc.loc[(tmc['tmc']==i)&(tmc['date']==date)].plot(ax=ax1,x='time', y='ATR_Counts', color='k',legend=None)\n",
    "    #dg.loc[dg['Station']==i].plot(ax=ax,x='hour', y='q75')\n",
    "    ax1.set_xlabel(\"Time of Day\",size=15)\n",
    "    ax1.set_ylabel('Flow (veh/15-min)', color='black',size=15)\n",
    "#     ax1.set_xticks(np.arange(0, 97, 4))\n",
    "#     ax1.set_xticklabels(tt, rotation=90, fontsize=25)\n",
    "#     ax1.set_xlim(0,96)\n",
    "    #plt.gcf().autofmt_xdate()\n",
    "    #ax1.xaxis.set_major_formatter(myFmt)\n",
    "    plt.legend([bpr[\"boxes\"][0],bpl[\"boxes\"][0]], ['ANN', 'FSTGCN'], edgecolor='black', loc=9, prop={'size': 15})\n",
    "    plt.legend(prop={'size': 15})\n",
    "    plt.title(\"TMC (%s)\" %(i),size=20)\n",
    "    fig.savefig('C:\\\\Users\\\\szahedi1\\\\Photos\\\\belt\\\\(%s)TMC (%s).png'%(date,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\Data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['Detector_ID']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df = pd.get_dummies(df, columns=[\"hour\",'quarter','year','month','weekday','day','roadname_up','roadname_dn',\n",
    "                                     'Detector_ID','dir','move','lanes_up','lanes_dn'] )\n",
    "dh = df[['Detector_ID']]\n",
    "dum_df = dum_df.join(dh).drop(['pair','measurement_tstamp','Up','Dn','Direction','xd_up','tmc_up','xd_dn','tmc_dn',\n",
    "                               'Intersection Name','date','dn_dir','movements'],axis=1)\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    i=i+1\n",
    "    train=dum_df.loc[dum_df['Detector_ID']!=loc]\n",
    "    test=dum_df.loc[dum_df['Detector_ID']==loc]\n",
    "    \n",
    "    X_train=train.drop(['count','Detector_ID'],axis=1)\n",
    "    X_test=test.drop(['count','Detector_ID'],axis=1)\n",
    "    \n",
    "    y_train=train['count']\n",
    "    y_test=test['count']\n",
    "    \n",
    "    regressor = xgb.XGBRegressor(n_estimators=100,reg_lambda=1, gamma=0,max_depth=3)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n",
    "    if (i==1):\n",
    "        results=res\n",
    "    else:\n",
    "        results=pd.concat([results,res])\n",
    "if (len(results.loc[results['actual']-results['count']!=0])==0):\n",
    "    print('Done')\n",
    "else:\n",
    "    print('there is a problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count\n",
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAPE\"])\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    result=result.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'count'], results.loc[results['Detector_ID']==loc,'pred'] ),\n",
    "                        \"XGBoost_MAPE\": (abs(results.loc[results['Detector_ID']==loc,'count']-results.loc[results['Detector_ID']==loc,'pred'])/results.loc[results['Detector_ID']==loc,'count']).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)\n",
    "    \n",
    "results['pro_act'] = results['count']/results['approach_count']\n",
    "results['pro_est'] = results['pred']/results['approach_count']\n",
    "\n",
    "# proportion\n",
    "result_p = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAE\"])\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    result_p=result_p.append({\"Detector_ID\":str(loc),\n",
    "        \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'pro_act'], results.loc[results['Detector_ID']==loc,'pro_est'] ),\n",
    "                        \"XGBoost_MAE\": (abs(results.loc[results['Detector_ID']==loc,'pro_act']-results.loc[results['Detector_ID']==loc,'pro_est'])).mean()*100\n",
    "                        },\n",
    "                       ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['count_diff']    = results['pred']-results['count']\n",
    "results['count_diff_h0'] = (results['approach_count']*results['pro_m'])-results['count']\n",
    "results['abs_count_diff']    = abs(results['count_diff'])\n",
    "results['abs_count_diff_h0'] = abs(results['count_diff_h0'])\n",
    "results.abs_count_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.abs_count_diff_h0.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[['Detector_ID','count_diff','abs_count_diff','abs_count_diff_h0','count']].groupby(['Detector_ID']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['abs_count_diff']/results[['Detector_ID','count_diff','abs_count_diff','count']].groupby(['Detector_ID']).mean()['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
