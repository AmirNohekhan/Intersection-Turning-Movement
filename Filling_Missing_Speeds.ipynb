{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import datetime as dt\n",
    "import functools\n",
    "import glob\n",
    "import keras\n",
    "import math\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import more_itertools as mit\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import statsmodels as sm\n",
    "import time\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from datetime import date, timedelta, datetime\n",
    "from itertools import chain, permutations\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import loadtxt, matrix\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from time import time, strftime, gmtime\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
    "from torch_geometric.datasets import Reddit\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\data_09_17_21.csv\", delimiter=',' ,dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop_duplicates(subset=['measurement_tstamp',\"pair\"],inplace=True,keep='first',ignore_index=True)\n",
    "#df=dc.copy()\n",
    "df = dc.copy()\n",
    "df.drop(['roadname_up','xd_up','xd_dn','tmc_up','tmc_dn','roadname_dn','Intersection Name'],axis=1,inplace=True)\n",
    "df=df.sort_values(by='speed_up', ascending=True).reset_index(drop=True).reset_index()\n",
    "# df = df.loc[df['l_veh'].notna()]\n",
    "# dg = df[['measurement_tstamp','Detector_ID','dir','pair']].groupby(['measurement_tstamp','Detector_ID',\n",
    "#                                                                     'dir']).count().reset_index()\n",
    "# dg = dg.rename(columns={\"pair\":\"approach_obs\"})\n",
    "# df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').set_index('index')\n",
    "# df['index']=df.index\n",
    "# df = df[df[\"movements\"]==df[\"approach_obs\"]]\n",
    "# df['h_veh'] = df['h_veh'].fillna(0)\n",
    "# df['count'] = df['h_veh'] + df['l_veh']\n",
    "# df = df.drop(['l_veh','h_veh'],axis=1)\n",
    "# dg = df.groupby(['measurement_tstamp','Detector_ID','dir']).sum()\n",
    "# dg=dg.reset_index()\n",
    "# dg = dg[['measurement_tstamp','Detector_ID','dir','count']]\n",
    "# dg = dg.rename(columns={\"count\":\"approach_count\"})\n",
    "# df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').set_index('index').drop(['approach_obs'],axis=1)\n",
    "df['index']=df.index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ref_speed_dn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time']=(df['hour']+df['quarter']/4)\n",
    "\n",
    "dum_df = pd.get_dummies(df, columns=['dir','Dn','year','month','weekday','time'] )\n",
    "#\n",
    "dum_df = dum_df.drop(['pair','measurement_tstamp','hour', 'quarter','day',\n",
    "                      'Up','Detector_ID','Direction','Exclusive_Lane',\n",
    "                      'lanes_up','lanes_dn','ref_speed_up','speed_dn','avg_speed_up','avg_speed_dn',\n",
    "                      \n",
    "                     'movements','move','index','l_veh','h_veh','speed_up'],axis=1)\n",
    "\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs=5\n",
    "\n",
    "train=dum_df.loc[dum_df['ref_speed_dn'].notna()]\n",
    "print(train.shape)\n",
    "test =dum_df\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "X_train=train.drop(['ref_speed_dn'],axis=1)\n",
    "print(X_train.shape)\n",
    "X_test=test.drop(['ref_speed_dn'],axis=1)\n",
    "print(X_test.shape)\n",
    "y_train=train['ref_speed_dn']\n",
    "print(y_train.shape)\n",
    "y_test=test['ref_speed_dn']\n",
    "print(y_test.shape)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, input_dim=224, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(128, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(128, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(128, use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "#model.add(Dropout(0.10))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae','mape'])\n",
    "history = model.fit(X_train, y_train, epochs=Epochs, batch_size=8096,  verbose=1, validation_data = (X_test, y_test))\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred'] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.merge(df[['pair','measurement_tstamp','Dn','pred']],on=['pair','measurement_tstamp','Dn'],how='left')\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.ref_speed_dn.fillna(dc.pred, inplace=True)\n",
    "dc= dc.drop(['pred'],axis=1)\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc[dc['ref_speed_dn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc.drop(['Unnamed: 0'],axis=1)\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.to_csv('C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\data_09_17_21.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=dum_df.loc[dum_df['speed_up'].notna()]\n",
    "test =dum_df\n",
    "print(train.shape,test.shape)\n",
    "\n",
    "\n",
    "X_train=train.drop(['speed_up'],axis=1)\n",
    "X_test=test.drop(['speed_up'],axis=1)\n",
    "\n",
    "y_train=train['speed_up']\n",
    "y_test=test['speed_up']\n",
    "\n",
    "regressor = xgb.XGBRegressor(n_estimators=50,reg_lambda=1, gamma=0,max_depth=9, learning_rate=0.1)\n",
    "regressor.fit(X_train, y_train)\n",
    "#res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=test.merge(pd.DataFrame(data={'actual':y_test,'pred':regressor.predict(X_test)}), how='left', left_index=True, right_index=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(res[['pred']], left_index=True, right_index=True,how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MAPE'] = abs(df['pred']-df['speed_up'])/df['speed_up']\n",
    "df.MAPE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dif.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.wide_to_long(res.reset_index(),  stubnames='Up_', i='index', j='Up')\n",
    "res = res.loc[res['Up_']==1]\n",
    "res = pd.wide_to_long(res.reset_index(),  stubnames='weekday_', i='index', j='weekday')\n",
    "res = res.loc[res['weekday_']==1]\n",
    "res = pd.wide_to_long(res.reset_index(),  stubnames='month_', i='index', j='month')\n",
    "res = res.loc[res['month_']==1]\n",
    "res = pd.wide_to_long(res.reset_index(),  stubnames='time_', i='index', j='time')\n",
    "res = res.loc[res['time_']==1]\n",
    "res = pd.wide_to_long(res.reset_index(),  stubnames='year_', i='index', j='year')\n",
    "res = res.loc[res['year_']==1]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.drop(['weekday_','month_','Up_','time_','year_'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['dir']=res['dir_NB']*4+res['dir_WB']*3+res['dir_EB']*2+res['dir_SB']\n",
    "res['dir'] = ['NB' if x ==4 else 'WB' if x ==3 else 'EB' if x ==2 else 'SB'for x in res[\"dir\"]]\n",
    "res = res.drop(['dir_NB','dir_WB','dir_EB','dir_SB'],axis=1)\n",
    "res = res.reset_index().drop(['index'],axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.reset_index().drop(['index','actual','speed_up'],axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.reset_index().drop(['actual','speed_up'],axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.merge(res,on=['year','month','weekday','time','Up','dir'],how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.wide_to_long(ds.reset_index(),  stubnames='Course_0', i='index', j='Course')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.wide_to_long(ds.reset_index(), stubnames='Course_',i='index',j='T')\n",
    "#not_dummy = ndf[ndf['T_'].ne(0)].reset_index(level='T').drop('T_',1)\n",
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (i==1):\n",
    "    results=res\n",
    "else:\n",
    "    results=pd.concat([results,res])\n",
    "if (len(results.loc[results['actual']-results['nextcount']!=0])==0):\n",
    "    print('Done')\n",
    "else:\n",
    "    print('there is a problem')\n",
    "# count\n",
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"XGBoost_R_Squared\",\"XGBoost_MAPE\",\"XGBoost_MAE\"])\n",
    "result=result.append({\"Detector_ID\":str(loc),\n",
    "                          \"XGBoost_R_Squared\":r2_score(results.loc[results['Detector_ID']==loc,'nextcount'], results.loc[results['Detector_ID']==loc,'pred'] ),\n",
    "                          \"XGBoost_MAPE\": (abs(results.loc[results['Detector_ID']==loc,'nextcount']-results.loc[results['Detector_ID']==loc,'pred'])/results.loc[results['Detector_ID']==loc,'nextcount']).mean()*100,\n",
    "                          \"XGBoost_MAE\": (abs(results.loc[results['Detector_ID']==loc,'nextcount']-results.loc[results['Detector_ID']==loc,'pred'])).mean()},\n",
    "                       ignore_index=True)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['l_veh'].notna()]\n",
    "dg = df[['measurement_tstamp','Detector_ID','dir','pair']].groupby(['measurement_tstamp','Detector_ID',\n",
    "                                                                    'dir']).count().reset_index()\n",
    "dg = dg.rename(columns={\"pair\":\"approach_obs\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left')\n",
    "df = df[df[\"movements\"]==df[\"approach_obs\"]]\n",
    "df['h_veh'] = df['h_veh'].fillna(0)\n",
    "df['count'] = df['h_veh'] + df['l_veh']\n",
    "df = df.drop(['l_veh','h_veh'],axis=1)\n",
    "dg = df.groupby(['measurement_tstamp','Detector_ID','dir']).sum().reset_index()\n",
    "dg = dg[['measurement_tstamp','Detector_ID','dir','count']]\n",
    "dg = dg.rename(columns={\"count\":\"approach_count\"})\n",
    "df = df.merge(dg, on=[\"Detector_ID\",\"measurement_tstamp\",\"dir\"], how='left').drop(['approach_obs'],axis=1)\n",
    "# df = df[df['speed_up'].notna()]\n",
    "# df = df[df['speed_dn'].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['approach_count'].notna())&(df['speed_up'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dc.copy()\n",
    "dg.drop(['Unnamed: 0','Unnamed: 0.1','roadname_up','xd_up','xd_dn','tmc_up','tmc_dn','roadname_dn','Intersection Name',\n",
    "        'l_veh','h_veh'],axis=1,inplace=True)\n",
    "dg.drop_duplicates(subset=['measurement_tstamp',\"pair\"],inplace=True,keep='first',ignore_index=True)\n",
    "dg['date'] = pd.to_datetime(dg['measurement_tstamp']).dt.date\n",
    "df = dg.merge(df[['pair','measurement_tstamp','count','approach_count']],on=['pair','measurement_tstamp'],how='left')\n",
    "df['measurement_tstamp'] = pd.to_datetime(df.measurement_tstamp)\n",
    "df['week'] = df.measurement_tstamp.dt.week\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.DataFrame()\n",
    "dg = df[['pair','measurement_tstamp','count']]\n",
    "dg = dg.rename(columns={'count':'nextcount'})\n",
    "dg['measurement_tstamp'] = pd.to_datetime(dg['measurement_tstamp'])\n",
    "dg['measurement_tstamp'] -= timedelta(hours=0, minutes=lag)\n",
    "df['measurement_tstamp'] = pd.to_datetime(df['measurement_tstamp'])\n",
    "df = df.merge(dg, on=['pair','measurement_tstamp'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of downstream volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dn_dir'] = ['NB' if x ==\"NB_TH\" else 'NB' if x ==\"WB_RT\" else 'NB' if x ==\"EB_LT\" else\n",
    "                'SB' if x ==\"SB_TH\" else 'SB' if x ==\"EB_RT\" else 'SB' if x ==\"WB_LT\" else\n",
    "                'EB' if x ==\"EB_TH\" else 'EB' if x ==\"NB_RT\" else 'EB' if x ==\"SB_LT\" else 'WB'for x in df[\"Direction\"]]\n",
    "dh = df[df['count'].notna()]\n",
    "dh = dh.groupby(['Detector_ID','measurement_tstamp','dn_dir']).count().reset_index()\n",
    "dh = dh[['Detector_ID','measurement_tstamp','dn_dir','pair']]\n",
    "dh = dh.rename(columns={'pair':'dn_moves'})\n",
    "dg = df.groupby(['Detector_ID','measurement_tstamp','dn_dir']).count().reset_index()\n",
    "dg = dg[['Detector_ID','measurement_tstamp','dn_dir','pair']]\n",
    "dg = dg.rename(columns={'pair':'dn_max_moves'})\n",
    "\n",
    "dg = dg.merge(dh,on=['Detector_ID','measurement_tstamp','dn_dir'],how='left')\n",
    "dg = dg[dg['dn_max_moves']==dg['dn_moves']]\n",
    "\n",
    "\n",
    "dg=df.merge(dg, how='inner', on=['Detector_ID','measurement_tstamp','dn_dir'])\n",
    "dg = dg[['Detector_ID','measurement_tstamp','dn_dir','count']].groupby(['Detector_ID','measurement_tstamp',\n",
    "                                                                        'dn_dir']).sum().reset_index()\n",
    "dg = dg.rename(columns={'count':'dn_count'})\n",
    "df = df.merge(dg, how='inner', on=['Detector_ID','measurement_tstamp','dn_dir'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df.dropna()\n",
    "dg['weekend'] = [0 if x <5 else 1 for x in dg[\"weekday\"]]\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = dg.dropna().groupby(['hour','Detector_ID','weekend','pair']).Dn.count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq['days'] = [5 if x ==0 else 2 for x in dq[\"weekend\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq['Dn1']=dq['Dn']/dq['days']/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq.sort_values(by=['Dn1']).tail(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq.loc[dq['Dn1']>30].groupby(['hour']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dq.loc[(dq['Dn1']>14)&(dq['weekend']==1)].pair.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdet = list(dq.loc[dq['Dn1']>14].Detector_ID.unique())\n",
    "listdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dg.pair.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.loc[dg['Detector_ID'].isin(listdet)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = dg.loc[(dg['hour']>6)&(dg['hour']<20)]\n",
    "dq = dg.groupby(['Detector_ID','Direction','hour']).pair.count().reset_index().sort_values(by='pair').reset_index()\n",
    "dq= dq[dq['pair']<56]\n",
    "dq['comp'] = dq['Detector_ID'].astype(int).astype(str) + dq['Direction'].astype(str) + dq['hour'].astype(str)\n",
    "dq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['comp'] = dg['Detector_ID'].astype(int).astype(str) + dg['Direction'].astype(str) + dg['hour'].astype(str)\n",
    "dg = dg.loc[~dg['comp'].isin(list(dq.comp.unique()))]\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = dg.groupby(['Detector_ID','Direction','hour','quarter','weekday']).pair.count().reset_index().sort_values(by='pair').reset_index()\n",
    "dg['comp'] = dg['Detector_ID'].astype(int).astype(str) + dg['Direction'].astype(str) + dg['weekday'].astype(str)+ dg['hour'].astype(str) + dg['quarter'].astype(str)\n",
    "dg = dg.sort_values(by=['pair','measurement_tstamp'])\n",
    "dq = dg.drop_duplicates(subset=['comp'],keep='first',ignore_index=True)\n",
    "dq = dq[['pair','measurement_tstamp']]\n",
    "dq['ind']=1\n",
    "dg = dg.merge(dq,on=['pair','measurement_tstamp'],how='left')\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dum_df = pd.get_dummies(dg, columns=[\"hour\",'quarter','weekday','dir','move','dn_dir'] )\n",
    "\n",
    "dum_df = dum_df.drop(['pair','measurement_tstamp','year','month','day',\n",
    "                      'Up','Dn','Direction',\n",
    "                      'lanes_up','lanes_dn',\n",
    "                     'movements','date','count','week','comp'],axis=1)\n",
    "\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns = [\"Detector_ID\",\"Training_Set_Size\",\"Validation_Set_Size\",\n",
    "                                 \"Train_MSE\",\"Validation_MSE\",\n",
    "                                 \"Train_MAE\",\"Validation_MAE\",\n",
    "                                 \"Train_MAPE\",\"Validation_MAPE\",\n",
    "                                 \"Train_R_Squared\",\"Validation_R_Squared\"])\n",
    "\n",
    "Epochs=50\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    train=dum_df.loc[(dum_df['Detector_ID']==loc)&(dum_df['ind']==1)]\n",
    "    test =dum_df.loc[(dum_df['Detector_ID']==loc)&(dum_df['ind']!=1)]\n",
    "    print(loc,train.shape,test.shape)\n",
    "\n",
    "    \n",
    "    X_train=train.drop(['Detector_ID','nextcount','ind'],axis=1)\n",
    "    X_test=test.drop(['Detector_ID','nextcount','ind'],axis=1)\n",
    "    \n",
    "    y_train=train['nextcount']\n",
    "    y_test=test['nextcount']\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train= scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_dim=46, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(256, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(256, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(256, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.10))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mse','mae','mape'])\n",
    "    history = model.fit(X_train, y_train, epochs=Epochs, batch_size=32,  verbose=1, validation_data = (X_test, y_test))\n",
    "    r2_train = 1 - (np.sum((np.asarray(y_train).reshape(y_train.shape[0],1) - model.predict(X_train))**2))/(np.sum((np.asarray(y_train).reshape(y_train.shape[0],1) - np.mean(y_train))**2))\n",
    "    r2_test = 1 - (np.sum((np.asarray(y_test).reshape(y_test.shape[0],1) - model.predict(X_test))**2))/(np.sum((np.asarray(y_test).reshape(y_test.shape[0],1) - np.mean(y_test))**2))\n",
    "    \n",
    "    \n",
    "    result = result.append({\"Detector_ID\":loc,\n",
    "                            \"Training_Set_Size\":X_train.shape[0],\n",
    "                            \"Validation_Set_Size\":X_test.shape[0],\n",
    "                            \"Train_MSE\": history.history['mse'][Epochs-1],\n",
    "                            \"Validation_MSE\": history.history['val_mse'][Epochs-1],\n",
    "                            \"Train_MAE\": history.history['mae'][Epochs-1],\n",
    "                            \"Validation_MAE\": history.history['val_mae'][Epochs-1],\n",
    "                            \"Train_MAPE\": history.history['mape'][Epochs-1],\n",
    "                            \"Validation_MAPE\": history.history['val_mape'][Epochs-1],\n",
    "                            \"Train_R_Squared\": r2_train,\n",
    "                            \"Validation_R_Squared\":r2_test},\n",
    "                           ignore_index=True)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = dg.loc[dg['ind']==1].copy()\n",
    "dq['nextproportion'] = dq['count']/dq['approach_count']\n",
    "dq['measurement_tstamp'] -= timedelta(hours=0, minutes=lag)\n",
    "dum_df = dg.loc[dg['ind']!=1].copy()\n",
    "dum_df = dum_df.merge(dq[['pair','hour','quarter','weekday','nextproportion']],on=['pair','hour','quarter','weekday'])\n",
    "dum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_df['count_naive'] = dum_df['approach_count']*dum_df['nextproportion']\n",
    "dum_df['count_diff_h0'] = (dum_df['count_naive']-dum_df['nextcount'])\n",
    "dum_df['Naive_MAE'] = abs(dum_df['count_diff_h0'])\n",
    "dum_df['Naive_MAPE'] = abs(dum_df['count_diff_h0'])/dum_df['nextcount']*100\n",
    "\n",
    "\n",
    "db = dum_df[['Detector_ID','Naive_MAPE','Naive_MAE']].groupby(['Detector_ID']).mean().reset_index()\n",
    "i=0\n",
    "result['Naive_R_Squared']=0\n",
    "for loc in dum_df.Detector_ID.unique().tolist():\n",
    "    result.iloc[i,4] = {scipy.stats.linregress(x=dum_df[dum_df['Detector_ID']==loc]['count_naive'], y=dum_df[dum_df['Detector_ID']==loc]['nextcount']).rvalue ** 2}\n",
    "    i+=1\n",
    "result['Detector_ID'] = pd.to_numeric(result['Detector_ID'])\n",
    "result= result.merge(db, on=['Detector_ID'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = pd.read_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\results(halfsmoothall).csv\", delimiter=',' ,dtype=None)\n",
    "dg = dg.loc[dg['Horizon']==0]\n",
    "dg.drop(['Horizon'],axis=1, inplace=True)\n",
    "results = result[[\"Detector_ID\",\"Training_Set_Size\",\"Validation_Set_Size\",\"Validation_R_Squared\",\"Validation_MAPE\",\"Validation_MAE\"]].merge(dg,on=['Detector_ID'],how='left')\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pd.DataFrame()\n",
    "diff['Detector_ID'] = results['Detector_ID']\n",
    "diff['R2_X_N']    =   results['XGBoost_R_Squared']    - results['Naive_R_Squared']\n",
    "diff['R2_NN_N']   =   results['Validation_R_Squared'] - results['Naive_R_Squared']\n",
    "diff['R2_NN_X']   =   results['Validation_R_Squared'] - results['XGBoost_R_Squared']\n",
    "diff['MAPE_X_N']  = -(results['XGBoost_MAPE']         - results['Naive_MAPE'])\n",
    "diff['MAPE_N_N']  = -(results['Validation_MAPE']      - results['Naive_MAPE'])\n",
    "diff['MAPE_NN_X'] = -(results['Validation_MAPE']      - results['XGBoost_MAPE'])\n",
    "diff['MAE_X_N']   = -(results['XGBoost_MAE']          - results['Naive_MAE'])\n",
    "diff['MAE_NN_N']  = -(results['Validation_MAE']       - results['Naive_MAE'])\n",
    "diff['MAE_NN_X']  = -(results['Validation_MAE']       - results['XGBoost_MAE'])\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
