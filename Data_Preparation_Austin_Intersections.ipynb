{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bisect\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "#import networkx as nx\n",
    "import os\n",
    "import random\n",
    "from random import randint\n",
    "import re\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.utils.extmath import cartesian\n",
    "pd.set_option('display.max_columns', 150)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dh = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\probe_int_map.csv\", delimiter=',')\n",
    "dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_year = 2018\n",
    "start_month = 1\n",
    "start_day = 1\n",
    "end_year = 2020\n",
    "end_month = 12\n",
    "end_day = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Graph = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Graph.csv\", delimiter=',' ,dtype=None)\n",
    "Graph.drop_duplicates(subset=['seg_id'],inplace=True,keep='first',ignore_index=True)\n",
    "Graph_Structure = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Graph_Structure.csv\", delimiter=',', \n",
    "                              dtype=None)\n",
    "Graph_Structure[\"pair\"] = \"(\" + Graph_Structure[\"Up\"].astype(str) + \",\" +Graph_Structure[\"Dn\"].astype(str) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Graph_Structure = Graph_Structure.rename(columns={\"Up\": \"seg_id\"})\n",
    "Graph_Structure = Graph_Structure.merge(Graph,on=[\"seg_id\"], how='left')\n",
    "Graph_Structure = Graph_Structure.rename(columns={\"seg_id\": \"Up\", \"roadname\":\"roadname_up\", \"Lanes\":\"lanes_up\", \n",
    "                                                  \"XD\":\"xd_up\", \"TMC\":\"tmc_up\"})\n",
    "Graph_Structure = Graph_Structure.rename(columns={\"Dn\": \"seg_id\"})\n",
    "Graph_Structure = Graph_Structure.merge(Graph,on=[\"seg_id\"], how='left')\n",
    "Graph_Structure = Graph_Structure.rename(columns={\"seg_id\": \"Dn\", \"roadname\":\"roadname_dn\", \"Lanes\":\"lanes_dn\", \n",
    "                                                  \"XD\":\"xd_dn\", \"TMC\":\"tmc_dn\"})\n",
    "Graph[['XD','seg_id']] = Graph[['XD','seg_id']].astype(int).astype(str)\n",
    "Graph_Structure[['xd_up','xd_dn','Up','Dn']] = Graph_Structure[['xd_up','xd_dn','Up','Dn']].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1 = Graph_Structure.pair.unique().tolist()\n",
    "l2 = []\n",
    "sdate = date(start_year, start_month, start_day)   # start date\n",
    "edate = date(end_year, end_month, end_day)   # end date\n",
    "delta = edate - sdate       # as timedelta\n",
    "for i in range(delta.days + 1):\n",
    "    day = sdate + timedelta(days=i)\n",
    "    l2.append(str(day))\n",
    "l3 = [ x for x in range(0,24) ]\n",
    "l4 = [ x for x in range(1,5) ]\n",
    "df=pd.DataFrame(cartesian((l1, l2, l3, l4)), columns=['pair', 'Date', 'hour','quarter'])\n",
    "df['Date'] = pd.to_datetime(df.Date)\n",
    "df['year']=df.Date.dt.year\n",
    "df['month']=df.Date.dt.month\n",
    "df['weekday'] = df.Date.dt.weekday\n",
    "df['day'] = df.Date.dt.day\n",
    "df[['hour','quarter']] = df[['hour','quarter']].astype('float64')\n",
    "df[\"minute\"] = (df[\"quarter\"]-1)*15\n",
    "df['measurement_tstamp'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute' ]])\n",
    "df = df.drop(['minute'], axis=1)\n",
    "df = df.merge(Graph_Structure, on=[\"pair\"])\n",
    "df = df.drop([\"Date\"], axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Speed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed_XD = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Speed\\\\Inrix_XD\\\\Readings.csv\", delimiter=',',\n",
    "                       dtype=None)\n",
    "speed_XD['measurement_tstamp'] = pd.to_datetime(speed_XD['measurement_tstamp'])\n",
    "speed_XD=speed_XD.drop_duplicates(subset=['xd_id','measurement_tstamp'])\n",
    "speed_XD['xd_id'] = speed_XD['xd_id'].astype(int).astype(str)\n",
    "speed_TMC = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Speed\\\\Inrix_TMC\\\\Readings.csv\", delimiter=',',\n",
    "                        dtype=None)\n",
    "speed_TMC['measurement_tstamp'] = pd.to_datetime(speed_TMC['measurement_tstamp'])\n",
    "speed_TMC=speed_TMC.drop_duplicates(subset=['tmc_code','measurement_tstamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speed_XD.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = df.copy()\n",
    "df = df.rename(columns={\"tmc_up\": \"tmc_code\"})\n",
    "speed_TMC = speed_TMC.rename(columns={\"speed\": \"speed_up\", \"average_speed\":\"avg_speed_up\",\n",
    "                                      \"reference_speed\":\"ref_speed_up\"})\n",
    "df = df.merge(speed_TMC,on=[\"tmc_code\",\"measurement_tstamp\"], how='left')\n",
    "df = df.rename(columns={\"tmc_code\":\"tmc_up\", \"tmc_dn\": \"tmc_code\"})\n",
    "speed_TMC.columns = speed_TMC.columns.str.replace(\"up\", \"dn\")\n",
    "df = df.merge(speed_TMC,on=[\"tmc_code\",\"measurement_tstamp\"], how='left')\n",
    "df = df.rename(columns={\"tmc_code\": \"tmc_dn\"})\n",
    "df = df.sort_values(by=[\"measurement_tstamp\",\"pair\"], ascending=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = dg.rename(columns={\"xd_up\": \"xd_id\"})\n",
    "speed_XD = speed_XD.rename(columns={\"speed\": \"speed_up\", \"average_speed\":\"avg_speed_up\",\"reference_speed\":\"ref_speed_up\"})\n",
    "dg = dg.merge(speed_XD,on=[\"xd_id\",\"measurement_tstamp\"], how='left')\n",
    "dg = dg.rename(columns={\"xd_id\":\"xd_up\", \"xd_dn\": \"xd_id\"})\n",
    "speed_XD.columns = speed_XD.columns.str.replace(\"up\", \"dn\")\n",
    "dg = dg.merge(speed_XD,on=[\"xd_id\",\"measurement_tstamp\"], how='left')\n",
    "dg = dg.rename(columns={\"xd_id\": \"xd_dn\"})\n",
    "dg = dg.sort_values(by=[\"measurement_tstamp\",\"pair\"], ascending=True)\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.set_index(['pair','measurement_tstamp'], inplace=True)\n",
    "df.update(dg.set_index(['pair','measurement_tstamp']))\n",
    "df=df.sort_values([\"pair\",\"measurement_tstamp\"])\n",
    "df = df.reset_index(drop=False)\n",
    "print(df.shape)\n",
    "#df = df.fillna(method='ffill') # filling nulls with preceding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counts_df = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Camera_Traffic_Counts.csv\", delimiter=',',dtype=None)\n",
    "det_list = Graph_Structure.Detector_ID.unique().tolist()\n",
    "Counts_df = Counts_df.loc[Counts_df[\"ATD Device ID\"].isin(det_list)]\n",
    "Counts_df['measurement_tstamp'] = pd.to_datetime(Counts_df[['Year', 'Month', 'Day', 'Hour', 'Minute' ]])\n",
    "Counts_df = Counts_df.drop([\"Record ID\",\"Read Date\",\"Speed StdDev\",\"Seconds in Zone StdDev\",\"Month\",\"Day\",\"Year\",\"Hour\",\n",
    "                            \"Minute\",\"Day of Week\",\"Bin Duration (Seconds)\"],axis=1).rename(columns={\"ATD Device ID\":\n",
    "                                                                                                     \"Detector_ID\"})\n",
    "Counts_df[\"dir\"] = [\"EB\" if x ==\"EASTBOUND\" else \"SB\" if x ==\"SOUTHBOUND\" else \"WB\" if x ==\"WESTBOUND\" else\n",
    "                    \"NB\" if x ==\"NORTHBOUND\" else \"0\" for x in Counts_df[\"Direction\"]]\n",
    "Counts_df[\"move\"] = [\"TH\" if x ==\"THRU\" else \"RT\" if x ==\"RIGHT TURN\" else \"LT\" if x ==\"LEFT TURN\" else\n",
    "                     0 for x in Counts_df[\"Movement\"]]\n",
    "Counts_df = Counts_df[Counts_df[\"dir\"]!=\"0\"]\n",
    "Counts_df = Counts_df[Counts_df[\"move\"]!=\"0\"]\n",
    "Counts_df[\"Direction\"] = Counts_df[\"dir\"].astype(str) + \"_\" + Counts_df[\"move\"].astype(str)\n",
    "Counts_df = Counts_df.drop([\"Movement\"], axis=1)\n",
    "#Counts_df = Counts_df.drop([\"dir\",\"move\",\"Movement\"], axis=1)\n",
    "\n",
    "Counts_df_h = Counts_df.copy()\n",
    "Counts_df_h = Counts_df_h.loc[Counts_df_h[\"Heavy Vehicle\"]==True]\n",
    "Counts_df_h = Counts_df_h.drop([\"Heavy Vehicle\",\"Intersection Name\",\"Speed Average (Miles Per Hour)\",\n",
    "                                \"Seconds in Zone Average\",\"dir\",\"move\"],axis=1).rename(columns={\"Volume\":\"h_veh\"})\n",
    "Counts_df = Counts_df.loc[Counts_df[\"Heavy Vehicle\"]==False]\n",
    "Counts_df = Counts_df.rename(columns={\"Volume\":\"l_veh\"}).drop([\"Heavy Vehicle\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Counts_df.shape)\n",
    "Counts_df = Counts_df.drop_duplicates(subset=['Detector_ID', 'Direction', 'measurement_tstamp'], keep='last')\n",
    "print(Counts_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(Counts_df_h.shape)\n",
    "Counts_df_h = Counts_df_h.drop_duplicates(subset=['Detector_ID', 'Direction', 'measurement_tstamp'], keep='last')\n",
    "print(Counts_df_h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(Counts_df, on=[\"Detector_ID\",\"Direction\",\"measurement_tstamp\"], how='left')\n",
    "print(df.shape)\n",
    "df = df.merge(Counts_df_h, on=[\"Detector_ID\",\"Direction\",\"measurement_tstamp\"], how='left')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop([\"Speed Average (Miles Per Hour)\",'Seconds in Zone Average'], axis=1)\n",
    "df[['dir', 'move']] = df['Direction'].str.split('_', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df['time_str'] = df[\"measurement_tstamp\"].astype(str)\n",
    "# list_na = df[df['speed_up'].isna()].time_str.unique().tolist()\n",
    "# df = df.loc[~df[\"time_str\"].isin(list_na)]\n",
    "# list_na = df[df['speed_dn'].isna()].time_str.unique().tolist()\n",
    "# df = df.loc[~df[\"time_str\"].isin(list_na)]\n",
    "\n",
    "allow = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\Allowable_Movements.csv\", delimiter=',',\n",
    "                        dtype=None)\n",
    "df = df.merge(allow, on=['Detector_ID','dir'], how='left')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = df.loc[(df['Detector_ID']==6814)]\n",
    "dh = df.loc[(df['Detector_ID']==6358)]\n",
    "dg = dg.merge(dh[['pair','measurement_tstamp','l_veh','h_veh']],on=['pair','measurement_tstamp'],how='left')\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg['l_veh_x'] = dg['l_veh_x'].fillna(dg['l_veh_y'])\n",
    "dg['h_veh_x'] = dg['h_veh_x'].fillna(dg['h_veh_y'])\n",
    "dg  = dg.drop(['l_veh_y','h_veh_y'],axis=1).rename(columns={'l_veh_x':'l_veh','h_veh_x':'h_veh'})\n",
    "df = df.loc[(df['Detector_ID']!=6358)&(df['Detector_ID']!=6814)]\n",
    "df = df.append(dg)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['Detector_ID','measurement_tstamp','pair'])\n",
    "df = df.rename(columns={'dir':'up_dir'})\n",
    "df['dn_dir'] = ['NB' if x ==\"NB_TH\" else 'NB' if x ==\"WB_RT\" else 'NB' if x ==\"EB_LT\" else\n",
    "                'SB' if x ==\"SB_TH\" else 'SB' if x ==\"EB_RT\" else 'SB' if x ==\"WB_LT\" else\n",
    "                'EB' if x ==\"EB_TH\" else 'EB' if x ==\"NB_RT\" else 'EB' if x ==\"SB_LT\" else 'WB'for x in df[\"Direction\"]]\n",
    "df.loc[df['h_veh'].isna(),'h_veh']=0\n",
    "df['tmcount'] = df['l_veh']+df['h_veh']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(df.groupby(['Detector_ID','measurement_tstamp','up_dir']).agg({'tmcount': lambda x: x.sum(skipna=False)}).reset_index().rename(columns={'tmcount':'up_vol'}),on=['Detector_ID','measurement_tstamp','up_dir'],how='left')\n",
    "df = df.merge(df.groupby(['Detector_ID','measurement_tstamp','dn_dir']).agg({'tmcount': lambda x: x.sum(skipna=False)}).reset_index().rename(columns={'tmcount':'dn_vol'}),on=['Detector_ID','measurement_tstamp','dn_dir'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dc = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\data_09_28_21.csv\", delimiter=',' ,dtype=None)\n",
    "dc.columns = [str(col) + '_dc' for col in dc.columns]\n",
    "dc = dc.rename(columns={'pair_dc':'pair','measurement_tstamp_dc':'measurement_tstamp'})\n",
    "dc['measurement_tstamp'] = pd.to_datetime(dc['measurement_tstamp'])\n",
    "df = df.merge(dc[['pair','measurement_tstamp','speed_up_dc','avg_speed_up_dc','ref_speed_up_dc','speed_dn_dc',\n",
    "                  'avg_speed_dn_dc','ref_speed_dn_dc']],on=['pair','measurement_tstamp'],how='left')\n",
    "df['speed_up'] = df['speed_up'].fillna(df['speed_up_dc'])\n",
    "df['speed_dn'] = df['speed_dn'].fillna(df['speed_dn_dc'])\n",
    "df['avg_speed_up'] = df['avg_speed_up'].fillna(df['avg_speed_up_dc'])\n",
    "df['avg_speed_dn'] = df['avg_speed_dn'].fillna(df['avg_speed_dn_dc'])\n",
    "df['ref_speed_up'] = df['ref_speed_up'].fillna(df['ref_speed_up_dc'])\n",
    "df['ref_speed_dn'] = df['ref_speed_dn'].fillna(df['ref_speed_dn_dc'])\n",
    "df = df.drop(['speed_up_dc','avg_speed_up_dc','ref_speed_up_dc','speed_dn_dc',\n",
    "              'avg_speed_dn_dc','ref_speed_dn_dc'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = df.copy()\n",
    "dg = df.drop(['pair','Dn','Direction','Exclusive_Lane','roadname_up','xd_up','tmc_up','roadname_dn','xd_dn',\n",
    "              'tmc_dn','lanes_dn','speed_dn','avg_speed_dn','ref_speed_dn','Intersection Name','l_veh','move',\n",
    "              'h_veh','movements','dn_dir','tmcount','dn_vol'],axis=1)\n",
    "dg = dg.rename(columns={'Up':'xd','lanes_up':'lanes','speed_up':'speed','avg_speed_up':'avg_speed',\n",
    "                        'ref_speed_up':'ref_speed','up_dir':'dir','up_vol':'vol'})\n",
    "dh = df.copy()\n",
    "dh = df.drop(['pair','Up','Direction','Exclusive_Lane','roadname_dn','xd_dn','tmc_dn','roadname_up','xd_up',\n",
    "              'tmc_up','lanes_up','speed_up','avg_speed_up','ref_speed_up','Intersection Name','l_veh','move',\n",
    "              'h_veh','movements','up_dir','tmcount','up_vol'],axis=1)\n",
    "dh = dh.rename(columns={'Dn':'xd','lanes_dn':'lanes','speed_dn':'speed','avg_speed_dn':'avg_speed',\n",
    "                        'ref_speed_dn':'ref_speed','dn_dir':'dir','dn_vol':'vol'})\n",
    "dg = pd.concat((dg,dh))\n",
    "dg = dg.drop_duplicates(subset=['measurement_tstamp','xd','Detector_ID'])\n",
    "dh = dg[['measurement_tstamp','xd','vol']].groupby(['measurement_tstamp','xd']).agg({'vol': lambda x: x.sum(skipna=False)})\n",
    "dg = dg.drop(['vol'],axis=1).merge(dh,on=['measurement_tstamp','xd'],how='left')\n",
    "dg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg['time'] = dg['hour']+dg['quarter']/4\n",
    "df['time'] = df['hour']+df['quarter']/4\n",
    "dh = dg[['xd','time','vol']].groupby(['xd','time']).mean().reset_index()\n",
    "dh = dh.interpolate()\n",
    "dh = dh.groupby(['xd'])['vol'].sum().reset_index()\n",
    "dh = dh.rename(columns={'vol':'aadt'})\n",
    "dh['aadt'] = dh['aadt'].astype(int)\n",
    "dg = dg.merge(dh,on=['xd'],how='left')\n",
    "df = df.merge(dg[['xd','aadt']].rename(columns={'xd':'xd_up'}),on=['xd_up'],how='left').rename(columns={'aadt':'aadt_up'})\n",
    "df = df.merge(dg[['xd','aadt']].rename(columns={'xd':'xd_dn'}),on=['xd_dn'],how='left').rename(columns={'aadt':'aadt_dn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(dh[['xd','aadt']].rename(columns={'xd':'xd_up'}),on=['xd_up'],how='left').rename(columns={'aadt':'aadt_up'})\n",
    "df = df.merge(dh[['xd','aadt']].rename(columns={'xd':'xd_dn'}),on=['xd_dn'],how='left').rename(columns={'aadt':'aadt_dn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg.to_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\xd_10_01_21.csv\", sep=',',index=False)\n",
    "df.to_csv(\"C:\\\\Users\\\\szahedi1\\\\Amir_Thesis\\\\data_10_01_21.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding probe tm counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\2020_data_10_26.csv\", delimiter=',' ,dtype={'pair':'object',\n",
    "'measurement_tstamp' : 'object',\n",
    "'hour' : 'int64',\n",
    "'quarter' : 'int64',\n",
    "'year' : 'int64',\n",
    "'month' : 'int64',\n",
    "'weekday' : 'int64',\n",
    "'day' : 'int64',\n",
    "'Up' : 'int64',\n",
    "'Dn' : 'int64',\n",
    "'Detector_ID' : 'int64',\n",
    "'Direction' : 'object',\n",
    "'Exclusive_Lane' : 'int64',\n",
    "'roadname_up' : 'object',\n",
    "'xd_up' : 'int64',\n",
    "'tmc_up' : 'object',\n",
    "'lanes_up' : 'int64',\n",
    "'roadname_dn' : 'object',\n",
    "'xd_dn' : 'int64',\n",
    "'tmc_dn' : 'object',\n",
    "'lanes_dn' : 'int64',\n",
    "'speed_up' : 'float64',\n",
    "'avg_speed_up' : 'float64',\n",
    "'ref_speed_up' : 'float64',\n",
    "'speed_dn' : 'float64',\n",
    "'avg_speed_dn' : 'float64',\n",
    "'ref_speed_dn' : 'float64',\n",
    "'Intersection Name' : 'object',\n",
    "'l_veh' : 'float64',\n",
    "'up_dir' : 'object',\n",
    "'move' : 'object',\n",
    "'h_veh' : 'float64',\n",
    "'movements' : 'int64',\n",
    "'dn_dir' : 'object',\n",
    "'tmcount' : 'float64',\n",
    "'up_vol' : 'float64',\n",
    "'dn_vol' : 'float64',\n",
    "'time' : 'float64',\n",
    "'aadt_up' : 'int64',\n",
    "'aadt_dn' : 'int64'\n",
    "})\n",
    "db = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\2020_xd_10_26.csv\", delimiter=',' ,dtype={'measurement_tstamp' : 'object',\n",
    "'hour' : 'int64',\n",
    "'quarter' : 'int64',\n",
    "'year' : 'int64',\n",
    "'month' : 'int64',\n",
    "'weekday' : 'int64',\n",
    "'day' : 'int64',\n",
    "'xd' : 'int64',\n",
    "'Detector_ID' : 'int64',\n",
    "'lanes' : 'int64',\n",
    "'speed' : 'float64',\n",
    "'avg_speed' : 'float64',\n",
    "'ref_speed' : 'float64',\n",
    "'dir' : 'object',\n",
    "'vol' : 'float64',\n",
    "'time' : 'float64',\n",
    "'aadt' : 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\probe_tm.csv\", delimiter=',')\n",
    "dh = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\probe_int_map.csv\", delimiter=',')\n",
    "dh = dh.rename(columns={'probeintname':'intersection_name'})\n",
    "dg = dg.merge(dh,on=['intersection_name'],how = 'left')\n",
    "dg = dg.drop(['Unnamed: 0','trip_id','intersection_id','intersection_name','movement_id','movement_name',\n",
    "              'intersection_offset_m','tt_sec','start_utc_ts','total_stop_duration_sec','movement_type',\n",
    "              'approach_speed_kph','stop_delay_sec','control_delay_sec','queue_delay_sec','approach_mseq_idx',\n",
    "              'movement_sequences','points','traj_idx','start_segment_idx','end_segment_idx','start_segment_offset_m',\n",
    "              'end_segment_offset_m','waypoint_freq_sec','ref_tt_sec','turn_group_id','split_failure_count','intname',\n",
    "             'movement_turn_maneuver','movement_dir'],\n",
    "             axis=1)\n",
    "dg[\"date\"] = dg['start_date_local'].str.split(\"T\", expand=True)[0]\n",
    "dg[\"time\"] = dg['start_date_local'].str.split(\"T\", expand=True)[1]\n",
    "dg[\"time\"] = dg['time'].str.split(\"-\", expand=True)[0]\n",
    "dg['measurement_tstamp'] = pd.to_datetime(dg['date'] + ' ' + dg['time'])\n",
    "dg['measurement_tstamp'] = dg['measurement_tstamp'].dt.floor('15T')\n",
    "dg = dg.rename(columns={'movement_inbound_dir':'dir_up','movement_outbound_dir':'dir_dn'})\n",
    "df['measurement_tstamp'] = pd.to_datetime(df['measurement_tstamp'])\n",
    "db['measurement_tstamp'] = pd.to_datetime(db['measurement_tstamp'])\n",
    "dq = dg.groupby(['Detector_ID','measurement_tstamp','dir_up','dir_dn']).count().reset_index().drop(['date','time'],axis=1)\n",
    "dq = dq.rename(columns={'start_date_local':'p_tmcount'})\n",
    "dq['dir_up'] = dq['dir_up'] + 'B'\n",
    "dq['dir_dn'] = dq['dir_dn'] + 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(dq,on=['Detector_ID','measurement_tstamp','dir_up','dir_dn'],how='left')\n",
    "df['p_tmcount'] = df['p_tmcount'].fillna(value=0)\n",
    "dh = df[['measurement_tstamp','Detector_ID','dir_up',\n",
    "         'p_tmcount']].groupby(['measurement_tstamp','dir_up',\n",
    "                                'Detector_ID']).agg({'p_tmcount': lambda x: x.sum(skipna=False)}).reset_index().rename(columns={'p_tmcount':'p_vol_up'})\n",
    "df = df.merge(dh,on=['measurement_tstamp','Detector_ID','dir_up'],how='left')\n",
    "\n",
    "dh = df[['measurement_tstamp','Detector_ID','dir_dn',\n",
    "         'p_tmcount']].groupby(['measurement_tstamp','dir_dn',\n",
    "                                'Detector_ID']).agg({'p_tmcount': lambda x: x.sum(skipna=False)}).reset_index().rename(columns={'p_tmcount':'p_vol_dn'})\n",
    "df = df.merge(dh,on=['measurement_tstamp','Detector_ID','dir_dn'],how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df['p_vol_dn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"D:\\\\Amir_Nohekhan\\\\2020_data_11_22.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\2020_data_11_22.csv\", delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"D:\\\\Amir_Nohekhan\\\\2020_xd_10_26.csv\", delimiter=',')\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = df.copy()\n",
    "dg = dg.rename(columns={'xd_up':'xd','p_vol_up':'p_vol'})\n",
    "db = db.merge(dg[['measurement_tstamp','xd','p_vol']],on=['measurement_tstamp','xd'],how='left')\n",
    "\n",
    "dg = df.copy()\n",
    "dg = dg.rename(columns={'xd_dn':'xd','p_vol_dn':'p_vol'})\n",
    "db = db.merge(dg[['measurement_tstamp','xd','p_vol']],on=['measurement_tstamp','xd'],how='left')\n",
    "db.p_vol_x = db.p_vol_x.fillna(db.p_vol_y)\n",
    "db = db.rename(columns={'p_vol_x':'p_vol'}).drop(['p_vol_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db[db['p_vol'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[(df['measurement_tstamp']=='2020-01-01 00:00:00')&(df['Detector_ID']==6192)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db[db['p_vol'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db.to_csv(\"D:\\\\Amir_Nohekhan\\\\2020_xd_11_22.csv\", sep=',',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
